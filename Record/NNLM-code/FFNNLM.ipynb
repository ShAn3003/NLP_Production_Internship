{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(train_path, word2number_dict, batch_size, n_step):\n",
    "    \n",
    "    all_input_batch = []\n",
    "    all_target_batch = []\n",
    "\n",
    "    with open(train_path, 'r', encoding='utf-8') as fr:\n",
    "        text = fr.readlines()\n",
    "    text = [line.strip() for line in text]\n",
    " \n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in text:\n",
    "     \n",
    "        wordlist = sen.split()\n",
    "  \n",
    "        if(len(wordlist)<n_step):continue\n",
    "  \n",
    "        for i,word in enumerate(wordlist):\n",
    "            if(i+n_step>=len(wordlist)):break\n",
    "            input= []\n",
    "            for j in range(n_step):\n",
    "                input.append(word2number_dict[wordlist[i+j]])\n",
    "            target =word2number_dict[wordlist[i+n_step]]\n",
    "   \n",
    "            input_batch.append(input)\n",
    "            target_batch.append(target)\n",
    "            \n",
    "    for i in range(len(input_batch)):\n",
    "        if i+batch_size>len(input_batch):\n",
    "            break\n",
    "        all_input_batch.append(input_batch[i:i+batch_size])\n",
    "        all_target_batch.append(target_batch[i:i+batch_size])\n",
    "  \n",
    "    return all_input_batch, all_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_batch_valid_test(train_path, word2number_dict, batch_size, n_step):\n",
    "    def word2number(n):\n",
    "        try:\n",
    "            return word2number_dict[n]\n",
    "        except:\n",
    "            return 1   #<unk_word>\n",
    "\n",
    "    all_input_batch = []\n",
    "    all_target_batch = []\n",
    "\n",
    "    with open(train_path, 'r', encoding='utf-8') as fr:\n",
    "        text = fr.readlines()\n",
    "    text = [line.strip() for line in text]\n",
    " \n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in text:\n",
    "     \n",
    "        wordlist = sen.split()\n",
    "  \n",
    "        if(len(wordlist)<n_step):continue\n",
    "  \n",
    "        for i in range(len(wordlist)):\n",
    "            if(i+n_step>=len(wordlist)):break\n",
    "            input= []\n",
    "            for j in range(n_step):\n",
    "                input.append(word2number(wordlist[i+j]))\n",
    "            target =word2number(wordlist[i+n_step])\n",
    "   \n",
    "            input_batch.append(input)\n",
    "            target_batch.append(target)\n",
    "            \n",
    "    for i in range(len(input_batch)):\n",
    "        if i+batch_size>len(input_batch):\n",
    "            break\n",
    "        all_input_batch.append(input_batch[i:i+batch_size])\n",
    "        all_target_batch.append(target_batch[i:i+batch_size])\n",
    "  \n",
    "    return all_input_batch, all_target_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dict(train_path):\n",
    "    with open(train_path, 'r', encoding='utf-8') as fr:\n",
    "        text = fr.readlines()\n",
    "    text = [n.strip() for n in text]\n",
    "    text = ' '.join(text).split(\" \")\n",
    "    text = list(set(text))\n",
    "    word2number_dict = {}\n",
    "    number2word_dict = {}\n",
    "    for i,word in enumerate(text,2):\n",
    "        word2number_dict[word] = i\n",
    "        number2word_dict[i] = word\n",
    "    word2number_dict[\"<pad>\"] = 0\n",
    "    number2word_dict[0] = \"<pad>\"\n",
    "    word2number_dict[\"<unk_word>\"] = 1\n",
    "    number2word_dict[1] = \"<unk_word>\"\n",
    "\n",
    "    return word2number_dict, number2word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim,n_step):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_step = n_step\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim*n_step, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.fc3 = nn.Linear(embed_dim*n_step, vocab_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embed(X) # X: [batch_size, n_step] -> [batch_size, n_step, embed_dim]\n",
    "        X = X.view(-1, self.embed_dim*self.n_step) # X: [batch_size, n_step, embed_dim] -> [batch_size, n_step*embed_dim]\n",
    "        tanh = self.tanh(self.fc1(X))# tanh: [batch_size, hidden_dim]\n",
    "        output = self.fc2(tanh)+self.fc3(X) # output: [batch_size, vocab_size]\n",
    "        return output\n",
    "    \n",
    "# n_step = 2 # number of steps, n-1 in paper\n",
    "# n_hidden = 2 # number of hidden size, h in paper\n",
    "# m = 2 # embedding size, m in paper\n",
    "\n",
    "\n",
    "\n",
    "# learn_rate = 0.001\n",
    "# all_epoch = 200 #the all epoch for training\n",
    "# save_checkpoint_epoch = 10 # save a checkpoint per save_checkpoint_epoch epochs\n",
    "# model = NNLM(vocab_size=100, embed_dim=m, hidden_dim=n_hidden,n_step=n_step)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    n_step = 5 # number of steps, n-1 in paper\n",
    "    n_hidden = 128 # number of hidden size, h in paper\n",
    "    m = 16 # embedding size, m in paper\n",
    " \n",
    "    \n",
    " \n",
    "    learn_rate = 0.001\n",
    "    all_epoch = 100 #the all epoch for training\n",
    "    save_checkpoint_epoch = 10 # save a checkpoint per save_checkpoint_epoch epochs\n",
    "    model = NNLM(vocab_size=n_class, embed_dim=m, hidden_dim=n_hidden,n_step=n_step)\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    # Training\n",
    "    batch_number = len(all_input_batch)\n",
    "    \n",
    "    for epoch in range(all_epoch):\n",
    "        count_batch = 0\n",
    "        for input_batch, target_batch in zip(all_input_batch, all_target_batch):\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output = model(input_batch)\n",
    "            # output: [batch_size, n_class]\n",
    "            loss = criterion(output, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ppl = math.exp(loss.item())\n",
    "            \n",
    "            if (count_batch + 1) % 50 == 0:\n",
    "                print('Epoch:', '%04d' % (epoch + 1), 'Batch:', '%02d' % (count_batch + 1), f'/{batch_number}',\n",
    "                      'loss =', '{:.6f}'.format(loss), 'ppl =', '{:.6}'.format(ppl))\n",
    "\n",
    "            count_batch += 1\n",
    "        \n",
    "        total_loss = 0\n",
    "        # valid after training one epoch      \n",
    "        total_valid = len(all_valid_target)*128\n",
    "        count_loss = 0\n",
    "        for input_batch, target_batch in zip(all_valid_batch, all_valid_target):\n",
    "            with torch.no_grad():\n",
    "                loss = criterion(model(input_batch), target_batch)\n",
    "                total_loss += loss.item()\n",
    "                count_loss += 1\n",
    "          \n",
    "        print(f'Valid {total_valid} samples after epoch:', '%04d' % (epoch + 1), 'loss =',\n",
    "                '{:.6f}'.format(total_loss / count_loss),\n",
    "                'ppl =', '{:.6}'.format(math.exp(total_loss / count_loss)))\n",
    "\n",
    "        if (epoch+1) % save_checkpoint_epoch == 0:\n",
    "            torch.save(model, f'models/nnlm_model_epoch{epoch+1}.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dictionary is: 7613\n",
      "The number of the train batch is: 68439\n",
      "\n",
      "Train###############\n",
      "NNLM(\n",
      "  (embed): Embedding(7613, 16)\n",
      "  (fc1): Linear(in_features=80, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=7613, bias=True)\n",
      "  (fc3): Linear(in_features=80, out_features=7613, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Batch: 50 /68439 loss = 2.727856 ppl = 15.3\n",
      "Epoch: 0001 Batch: 100 /68439 loss = 0.831484 ppl = 2.29673\n",
      "Epoch: 0001 Batch: 150 /68439 loss = 0.538542 ppl = 1.71351\n",
      "Epoch: 0001 Batch: 200 /68439 loss = 0.442267 ppl = 1.55623\n",
      "Epoch: 0001 Batch: 250 /68439 loss = 0.347810 ppl = 1.41596\n",
      "Epoch: 0001 Batch: 300 /68439 loss = 0.335809 ppl = 1.39907\n",
      "Epoch: 0001 Batch: 350 /68439 loss = 0.315843 ppl = 1.37141\n",
      "Epoch: 0001 Batch: 400 /68439 loss = 0.265594 ppl = 1.30421\n",
      "Epoch: 0001 Batch: 450 /68439 loss = 0.192043 ppl = 1.21172\n",
      "Epoch: 0001 Batch: 500 /68439 loss = 0.262571 ppl = 1.30027\n",
      "Epoch: 0001 Batch: 550 /68439 loss = 0.284580 ppl = 1.3292\n",
      "Epoch: 0001 Batch: 600 /68439 loss = 0.205516 ppl = 1.22816\n",
      "Epoch: 0001 Batch: 650 /68439 loss = 0.229173 ppl = 1.25756\n",
      "Epoch: 0001 Batch: 700 /68439 loss = 0.211390 ppl = 1.23539\n",
      "Epoch: 0001 Batch: 750 /68439 loss = 0.186549 ppl = 1.20508\n",
      "Epoch: 0001 Batch: 800 /68439 loss = 0.204186 ppl = 1.22653\n",
      "Epoch: 0001 Batch: 850 /68439 loss = 0.251151 ppl = 1.2855\n",
      "Epoch: 0001 Batch: 900 /68439 loss = 0.223120 ppl = 1.24997\n",
      "Epoch: 0001 Batch: 950 /68439 loss = 0.132812 ppl = 1.14203\n",
      "Epoch: 0001 Batch: 1000 /68439 loss = 0.171101 ppl = 1.18661\n",
      "Epoch: 0001 Batch: 1050 /68439 loss = 0.203094 ppl = 1.22519\n",
      "Epoch: 0001 Batch: 1100 /68439 loss = 0.211143 ppl = 1.23509\n",
      "Epoch: 0001 Batch: 1150 /68439 loss = 0.227220 ppl = 1.25511\n",
      "Epoch: 0001 Batch: 1200 /68439 loss = 0.218519 ppl = 1.24423\n",
      "Epoch: 0001 Batch: 1250 /68439 loss = 0.223571 ppl = 1.25054\n",
      "Epoch: 0001 Batch: 1300 /68439 loss = 0.165777 ppl = 1.18031\n",
      "Epoch: 0001 Batch: 1350 /68439 loss = 0.227697 ppl = 1.25571\n",
      "Epoch: 0001 Batch: 1400 /68439 loss = 0.188418 ppl = 1.20734\n",
      "Epoch: 0001 Batch: 1450 /68439 loss = 0.183810 ppl = 1.20179\n",
      "Epoch: 0001 Batch: 1500 /68439 loss = 0.167703 ppl = 1.18258\n",
      "Epoch: 0001 Batch: 1550 /68439 loss = 0.180672 ppl = 1.19802\n",
      "Epoch: 0001 Batch: 1600 /68439 loss = 0.208215 ppl = 1.23148\n",
      "Epoch: 0001 Batch: 1650 /68439 loss = 0.236485 ppl = 1.26679\n",
      "Epoch: 0001 Batch: 1700 /68439 loss = 0.219130 ppl = 1.24499\n",
      "Epoch: 0001 Batch: 1750 /68439 loss = 0.200802 ppl = 1.22238\n",
      "Epoch: 0001 Batch: 1800 /68439 loss = 0.177818 ppl = 1.19461\n",
      "Epoch: 0001 Batch: 1850 /68439 loss = 0.176898 ppl = 1.19351\n",
      "Epoch: 0001 Batch: 1900 /68439 loss = 0.207955 ppl = 1.23116\n",
      "Epoch: 0001 Batch: 1950 /68439 loss = 0.169376 ppl = 1.18457\n",
      "Epoch: 0001 Batch: 2000 /68439 loss = 0.184496 ppl = 1.20261\n",
      "Epoch: 0001 Batch: 2050 /68439 loss = 0.150193 ppl = 1.16206\n",
      "Epoch: 0001 Batch: 2100 /68439 loss = 0.218193 ppl = 1.24383\n",
      "Epoch: 0001 Batch: 2150 /68439 loss = 0.190104 ppl = 1.20937\n",
      "Epoch: 0001 Batch: 2200 /68439 loss = 0.221257 ppl = 1.24764\n",
      "Epoch: 0001 Batch: 2250 /68439 loss = 0.174881 ppl = 1.1911\n",
      "Epoch: 0001 Batch: 2300 /68439 loss = 0.192887 ppl = 1.21275\n",
      "Epoch: 0001 Batch: 2350 /68439 loss = 0.172184 ppl = 1.1879\n",
      "Epoch: 0001 Batch: 2400 /68439 loss = 0.164605 ppl = 1.17893\n",
      "Epoch: 0001 Batch: 2450 /68439 loss = 0.177672 ppl = 1.19443\n",
      "Epoch: 0001 Batch: 2500 /68439 loss = 0.199132 ppl = 1.22034\n",
      "Epoch: 0001 Batch: 2550 /68439 loss = 0.197233 ppl = 1.21803\n",
      "Epoch: 0001 Batch: 2600 /68439 loss = 0.168229 ppl = 1.18321\n",
      "Epoch: 0001 Batch: 2650 /68439 loss = 0.205662 ppl = 1.22834\n",
      "Epoch: 0001 Batch: 2700 /68439 loss = 0.144026 ppl = 1.15491\n",
      "Epoch: 0001 Batch: 2750 /68439 loss = 0.192460 ppl = 1.21223\n",
      "Epoch: 0001 Batch: 2800 /68439 loss = 0.155982 ppl = 1.16881\n",
      "Epoch: 0001 Batch: 2850 /68439 loss = 0.159325 ppl = 1.17272\n",
      "Epoch: 0001 Batch: 2900 /68439 loss = 0.182757 ppl = 1.20052\n",
      "Epoch: 0001 Batch: 2950 /68439 loss = 0.163231 ppl = 1.17731\n",
      "Epoch: 0001 Batch: 3000 /68439 loss = 0.193946 ppl = 1.21403\n",
      "Epoch: 0001 Batch: 3050 /68439 loss = 0.178445 ppl = 1.19536\n",
      "Epoch: 0001 Batch: 3100 /68439 loss = 0.181648 ppl = 1.19919\n",
      "Epoch: 0001 Batch: 3150 /68439 loss = 0.159309 ppl = 1.1727\n",
      "Epoch: 0001 Batch: 3200 /68439 loss = 0.201226 ppl = 1.2229\n",
      "Epoch: 0001 Batch: 3250 /68439 loss = 0.176532 ppl = 1.19307\n",
      "Epoch: 0001 Batch: 3300 /68439 loss = 0.232156 ppl = 1.26132\n",
      "Epoch: 0001 Batch: 3350 /68439 loss = 0.172820 ppl = 1.18865\n",
      "Epoch: 0001 Batch: 3400 /68439 loss = 0.158683 ppl = 1.17197\n",
      "Epoch: 0001 Batch: 3450 /68439 loss = 0.184199 ppl = 1.20225\n",
      "Epoch: 0001 Batch: 3500 /68439 loss = 0.115245 ppl = 1.12215\n",
      "Epoch: 0001 Batch: 3550 /68439 loss = 0.157368 ppl = 1.17043\n",
      "Epoch: 0001 Batch: 3600 /68439 loss = 0.197000 ppl = 1.21774\n",
      "Epoch: 0001 Batch: 3650 /68439 loss = 0.152854 ppl = 1.16515\n",
      "Epoch: 0001 Batch: 3700 /68439 loss = 0.151119 ppl = 1.16314\n",
      "Epoch: 0001 Batch: 3750 /68439 loss = 0.164648 ppl = 1.17898\n",
      "Epoch: 0001 Batch: 3800 /68439 loss = 0.177117 ppl = 1.19377\n",
      "Epoch: 0001 Batch: 3850 /68439 loss = 0.165622 ppl = 1.18013\n",
      "Epoch: 0001 Batch: 3900 /68439 loss = 0.111170 ppl = 1.11758\n",
      "Epoch: 0001 Batch: 3950 /68439 loss = 0.204958 ppl = 1.22747\n",
      "Epoch: 0001 Batch: 4000 /68439 loss = 0.156052 ppl = 1.16889\n",
      "Epoch: 0001 Batch: 4050 /68439 loss = 0.203384 ppl = 1.22554\n",
      "Epoch: 0001 Batch: 4100 /68439 loss = 0.182501 ppl = 1.20022\n",
      "Epoch: 0001 Batch: 4150 /68439 loss = 0.184678 ppl = 1.20283\n",
      "Epoch: 0001 Batch: 4200 /68439 loss = 0.194792 ppl = 1.21506\n",
      "Epoch: 0001 Batch: 4250 /68439 loss = 0.126089 ppl = 1.13438\n",
      "Epoch: 0001 Batch: 4300 /68439 loss = 0.118819 ppl = 1.12617\n",
      "Epoch: 0001 Batch: 4350 /68439 loss = 0.169961 ppl = 1.18526\n",
      "Epoch: 0001 Batch: 4400 /68439 loss = 0.215675 ppl = 1.2407\n",
      "Epoch: 0001 Batch: 4450 /68439 loss = 0.172252 ppl = 1.18798\n",
      "Epoch: 0001 Batch: 4500 /68439 loss = 0.162036 ppl = 1.1759\n",
      "Epoch: 0001 Batch: 4550 /68439 loss = 0.212582 ppl = 1.23687\n",
      "Epoch: 0001 Batch: 4600 /68439 loss = 0.194167 ppl = 1.2143\n",
      "Epoch: 0001 Batch: 4650 /68439 loss = 0.166233 ppl = 1.18085\n",
      "Epoch: 0001 Batch: 4700 /68439 loss = 0.191938 ppl = 1.2116\n",
      "Epoch: 0001 Batch: 4750 /68439 loss = 0.161040 ppl = 1.17473\n",
      "Epoch: 0001 Batch: 4800 /68439 loss = 0.149576 ppl = 1.16134\n",
      "Epoch: 0001 Batch: 4850 /68439 loss = 0.131403 ppl = 1.14043\n",
      "Epoch: 0001 Batch: 4900 /68439 loss = 0.164306 ppl = 1.17858\n",
      "Epoch: 0001 Batch: 4950 /68439 loss = 0.201797 ppl = 1.2236\n",
      "Epoch: 0001 Batch: 5000 /68439 loss = 0.218000 ppl = 1.24359\n",
      "Epoch: 0001 Batch: 5050 /68439 loss = 0.218192 ppl = 1.24383\n",
      "Epoch: 0001 Batch: 5100 /68439 loss = 0.158253 ppl = 1.17146\n",
      "Epoch: 0001 Batch: 5150 /68439 loss = 0.149910 ppl = 1.16173\n",
      "Epoch: 0001 Batch: 5200 /68439 loss = 0.167200 ppl = 1.18199\n",
      "Epoch: 0001 Batch: 5250 /68439 loss = 0.168338 ppl = 1.18334\n",
      "Epoch: 0001 Batch: 5300 /68439 loss = 0.228486 ppl = 1.2567\n",
      "Epoch: 0001 Batch: 5350 /68439 loss = 0.187586 ppl = 1.20633\n",
      "Epoch: 0001 Batch: 5400 /68439 loss = 0.142774 ppl = 1.15347\n",
      "Epoch: 0001 Batch: 5450 /68439 loss = 0.128854 ppl = 1.13752\n",
      "Epoch: 0001 Batch: 5500 /68439 loss = 0.168808 ppl = 1.18389\n",
      "Epoch: 0001 Batch: 5550 /68439 loss = 0.193269 ppl = 1.21321\n",
      "Epoch: 0001 Batch: 5600 /68439 loss = 0.068374 ppl = 1.07077\n",
      "Epoch: 0001 Batch: 5650 /68439 loss = 0.240508 ppl = 1.27189\n",
      "Epoch: 0001 Batch: 5700 /68439 loss = 0.186833 ppl = 1.20543\n",
      "Epoch: 0001 Batch: 5750 /68439 loss = 0.210205 ppl = 1.23393\n",
      "Epoch: 0001 Batch: 5800 /68439 loss = 0.236920 ppl = 1.26734\n",
      "Epoch: 0001 Batch: 5850 /68439 loss = 0.174449 ppl = 1.19059\n",
      "Epoch: 0001 Batch: 5900 /68439 loss = 0.144985 ppl = 1.15602\n",
      "Epoch: 0001 Batch: 5950 /68439 loss = 0.198961 ppl = 1.22013\n",
      "Epoch: 0001 Batch: 6000 /68439 loss = 0.163131 ppl = 1.17719\n",
      "Epoch: 0001 Batch: 6050 /68439 loss = 0.221103 ppl = 1.24745\n",
      "Epoch: 0001 Batch: 6100 /68439 loss = 0.221631 ppl = 1.24811\n",
      "Epoch: 0001 Batch: 6150 /68439 loss = 0.202519 ppl = 1.22448\n",
      "Epoch: 0001 Batch: 6200 /68439 loss = 0.173842 ppl = 1.18987\n",
      "Epoch: 0001 Batch: 6250 /68439 loss = 0.190973 ppl = 1.21043\n",
      "Epoch: 0001 Batch: 6300 /68439 loss = 0.212719 ppl = 1.23704\n",
      "Epoch: 0001 Batch: 6350 /68439 loss = 0.167670 ppl = 1.18255\n",
      "Epoch: 0001 Batch: 6400 /68439 loss = 0.174211 ppl = 1.19031\n",
      "Epoch: 0001 Batch: 6450 /68439 loss = 0.204662 ppl = 1.22711\n",
      "Epoch: 0001 Batch: 6500 /68439 loss = 0.164053 ppl = 1.17828\n",
      "Epoch: 0001 Batch: 6550 /68439 loss = 0.151329 ppl = 1.16338\n",
      "Epoch: 0001 Batch: 6600 /68439 loss = 0.193087 ppl = 1.21299\n",
      "Epoch: 0001 Batch: 6650 /68439 loss = 0.148173 ppl = 1.15971\n",
      "Epoch: 0001 Batch: 6700 /68439 loss = 0.179972 ppl = 1.19718\n",
      "Epoch: 0001 Batch: 6750 /68439 loss = 0.191380 ppl = 1.21092\n",
      "Epoch: 0001 Batch: 6800 /68439 loss = 0.138320 ppl = 1.14834\n",
      "Epoch: 0001 Batch: 6850 /68439 loss = 0.155294 ppl = 1.168\n",
      "Epoch: 0001 Batch: 6900 /68439 loss = 0.167676 ppl = 1.18255\n",
      "Epoch: 0001 Batch: 6950 /68439 loss = 0.156548 ppl = 1.16947\n",
      "Epoch: 0001 Batch: 7000 /68439 loss = 0.142135 ppl = 1.15273\n",
      "Epoch: 0001 Batch: 7050 /68439 loss = 0.183697 ppl = 1.20165\n",
      "Epoch: 0001 Batch: 7100 /68439 loss = 0.172500 ppl = 1.18827\n",
      "Epoch: 0001 Batch: 7150 /68439 loss = 0.176753 ppl = 1.19334\n",
      "Epoch: 0001 Batch: 7200 /68439 loss = 0.173704 ppl = 1.1897\n",
      "Epoch: 0001 Batch: 7250 /68439 loss = 0.198897 ppl = 1.22006\n",
      "Epoch: 0001 Batch: 7300 /68439 loss = 0.132284 ppl = 1.14143\n",
      "Epoch: 0001 Batch: 7350 /68439 loss = 0.162356 ppl = 1.17628\n",
      "Epoch: 0001 Batch: 7400 /68439 loss = 0.164767 ppl = 1.17912\n",
      "Epoch: 0001 Batch: 7450 /68439 loss = 0.206772 ppl = 1.2297\n",
      "Epoch: 0001 Batch: 7500 /68439 loss = 0.191255 ppl = 1.21077\n",
      "Epoch: 0001 Batch: 7550 /68439 loss = 0.165656 ppl = 1.18017\n",
      "Epoch: 0001 Batch: 7600 /68439 loss = 0.179225 ppl = 1.19629\n",
      "Epoch: 0001 Batch: 7650 /68439 loss = 0.180892 ppl = 1.19829\n",
      "Epoch: 0001 Batch: 7700 /68439 loss = 0.187684 ppl = 1.20645\n",
      "Epoch: 0001 Batch: 7750 /68439 loss = 0.180379 ppl = 1.19767\n",
      "Epoch: 0001 Batch: 7800 /68439 loss = 0.178859 ppl = 1.19585\n",
      "Epoch: 0001 Batch: 7850 /68439 loss = 0.153159 ppl = 1.16551\n",
      "Epoch: 0001 Batch: 7900 /68439 loss = 0.187684 ppl = 1.20645\n",
      "Epoch: 0001 Batch: 7950 /68439 loss = 0.175903 ppl = 1.19232\n",
      "Epoch: 0001 Batch: 8000 /68439 loss = 0.199398 ppl = 1.22067\n",
      "Epoch: 0001 Batch: 8050 /68439 loss = 0.161775 ppl = 1.1756\n",
      "Epoch: 0001 Batch: 8100 /68439 loss = 0.218212 ppl = 1.24385\n",
      "Epoch: 0001 Batch: 8150 /68439 loss = 0.194286 ppl = 1.21444\n",
      "Epoch: 0001 Batch: 8200 /68439 loss = 0.182614 ppl = 1.20035\n",
      "Epoch: 0001 Batch: 8250 /68439 loss = 0.216412 ppl = 1.24161\n",
      "Epoch: 0001 Batch: 8300 /68439 loss = 0.098551 ppl = 1.10357\n",
      "Epoch: 0001 Batch: 8350 /68439 loss = 0.200668 ppl = 1.22222\n",
      "Epoch: 0001 Batch: 8400 /68439 loss = 0.224894 ppl = 1.25219\n",
      "Epoch: 0001 Batch: 8450 /68439 loss = 0.188203 ppl = 1.20708\n",
      "Epoch: 0001 Batch: 8500 /68439 loss = 0.230404 ppl = 1.25911\n",
      "Epoch: 0001 Batch: 8550 /68439 loss = 0.164363 ppl = 1.17864\n",
      "Epoch: 0001 Batch: 8600 /68439 loss = 0.189264 ppl = 1.20836\n",
      "Epoch: 0001 Batch: 8650 /68439 loss = 0.219453 ppl = 1.2454\n",
      "Epoch: 0001 Batch: 8700 /68439 loss = 0.246194 ppl = 1.27915\n",
      "Epoch: 0001 Batch: 8750 /68439 loss = 0.225960 ppl = 1.25353\n",
      "Epoch: 0001 Batch: 8800 /68439 loss = 0.124599 ppl = 1.13269\n",
      "Epoch: 0001 Batch: 8850 /68439 loss = 0.187261 ppl = 1.20594\n",
      "Epoch: 0001 Batch: 8900 /68439 loss = 0.181886 ppl = 1.19948\n",
      "Epoch: 0001 Batch: 8950 /68439 loss = 0.162796 ppl = 1.1768\n",
      "Epoch: 0001 Batch: 9000 /68439 loss = 0.198320 ppl = 1.21935\n",
      "Epoch: 0001 Batch: 9050 /68439 loss = 0.136593 ppl = 1.14636\n",
      "Epoch: 0001 Batch: 9100 /68439 loss = 0.155114 ppl = 1.16779\n",
      "Epoch: 0001 Batch: 9150 /68439 loss = 0.096455 ppl = 1.10126\n",
      "Epoch: 0001 Batch: 9200 /68439 loss = 0.190091 ppl = 1.20936\n",
      "Epoch: 0001 Batch: 9250 /68439 loss = 0.156464 ppl = 1.16937\n",
      "Epoch: 0001 Batch: 9300 /68439 loss = 0.257161 ppl = 1.29325\n",
      "Epoch: 0001 Batch: 9350 /68439 loss = 0.146928 ppl = 1.15827\n",
      "Epoch: 0001 Batch: 9400 /68439 loss = 0.133605 ppl = 1.14294\n",
      "Epoch: 0001 Batch: 9450 /68439 loss = 0.148356 ppl = 1.15993\n",
      "Epoch: 0001 Batch: 9500 /68439 loss = 0.154539 ppl = 1.16712\n",
      "Epoch: 0001 Batch: 9550 /68439 loss = 0.210880 ppl = 1.23476\n",
      "Epoch: 0001 Batch: 9600 /68439 loss = 0.194219 ppl = 1.21436\n",
      "Epoch: 0001 Batch: 9650 /68439 loss = 0.159267 ppl = 1.17265\n",
      "Epoch: 0001 Batch: 9700 /68439 loss = 0.189074 ppl = 1.20813\n",
      "Epoch: 0001 Batch: 9750 /68439 loss = 0.181860 ppl = 1.19945\n",
      "Epoch: 0001 Batch: 9800 /68439 loss = 0.174795 ppl = 1.191\n",
      "Epoch: 0001 Batch: 9850 /68439 loss = 0.180864 ppl = 1.19825\n",
      "Epoch: 0001 Batch: 9900 /68439 loss = 0.193759 ppl = 1.2138\n",
      "Epoch: 0001 Batch: 9950 /68439 loss = 0.189000 ppl = 1.20804\n",
      "Epoch: 0001 Batch: 10000 /68439 loss = 0.189402 ppl = 1.20853\n",
      "Epoch: 0001 Batch: 10050 /68439 loss = 0.200475 ppl = 1.22198\n",
      "Epoch: 0001 Batch: 10100 /68439 loss = 0.198434 ppl = 1.21949\n",
      "Epoch: 0001 Batch: 10150 /68439 loss = 0.137125 ppl = 1.14697\n",
      "Epoch: 0001 Batch: 10200 /68439 loss = 0.186289 ppl = 1.20477\n",
      "Epoch: 0001 Batch: 10250 /68439 loss = 0.221782 ppl = 1.2483\n",
      "Epoch: 0001 Batch: 10300 /68439 loss = 0.212348 ppl = 1.23658\n",
      "Epoch: 0001 Batch: 10350 /68439 loss = 0.181743 ppl = 1.19931\n",
      "Epoch: 0001 Batch: 10400 /68439 loss = 0.193780 ppl = 1.21383\n",
      "Epoch: 0001 Batch: 10450 /68439 loss = 0.199600 ppl = 1.22091\n",
      "Epoch: 0001 Batch: 10500 /68439 loss = 0.164284 ppl = 1.17855\n",
      "Epoch: 0001 Batch: 10550 /68439 loss = 0.183301 ppl = 1.20118\n",
      "Epoch: 0001 Batch: 10600 /68439 loss = 0.136414 ppl = 1.14616\n",
      "Epoch: 0001 Batch: 10650 /68439 loss = 0.198368 ppl = 1.21941\n",
      "Epoch: 0001 Batch: 10700 /68439 loss = 0.199410 ppl = 1.22068\n",
      "Epoch: 0001 Batch: 10750 /68439 loss = 0.176485 ppl = 1.19302\n",
      "Epoch: 0001 Batch: 10800 /68439 loss = 0.146185 ppl = 1.15741\n",
      "Epoch: 0001 Batch: 10850 /68439 loss = 0.165900 ppl = 1.18046\n",
      "Epoch: 0001 Batch: 10900 /68439 loss = 0.210220 ppl = 1.23395\n",
      "Epoch: 0001 Batch: 10950 /68439 loss = 0.177682 ppl = 1.19445\n",
      "Epoch: 0001 Batch: 11000 /68439 loss = 0.144119 ppl = 1.15502\n",
      "Epoch: 0001 Batch: 11050 /68439 loss = 0.199345 ppl = 1.2206\n",
      "Epoch: 0001 Batch: 11100 /68439 loss = 0.194343 ppl = 1.21451\n",
      "Epoch: 0001 Batch: 11150 /68439 loss = 0.189994 ppl = 1.20924\n",
      "Epoch: 0001 Batch: 11200 /68439 loss = 0.201910 ppl = 1.22374\n",
      "Epoch: 0001 Batch: 11250 /68439 loss = 0.166546 ppl = 1.18122\n",
      "Epoch: 0001 Batch: 11300 /68439 loss = 0.176425 ppl = 1.19295\n",
      "Epoch: 0001 Batch: 11350 /68439 loss = 0.131474 ppl = 1.14051\n",
      "Epoch: 0001 Batch: 11400 /68439 loss = 0.093537 ppl = 1.09805\n",
      "Epoch: 0001 Batch: 11450 /68439 loss = 0.179117 ppl = 1.19616\n",
      "Epoch: 0001 Batch: 11500 /68439 loss = 0.190223 ppl = 1.20952\n",
      "Epoch: 0001 Batch: 11550 /68439 loss = 0.188841 ppl = 1.20785\n",
      "Epoch: 0001 Batch: 11600 /68439 loss = 0.175277 ppl = 1.19158\n",
      "Epoch: 0001 Batch: 11650 /68439 loss = 0.174734 ppl = 1.19093\n",
      "Epoch: 0001 Batch: 11700 /68439 loss = 0.183825 ppl = 1.20181\n",
      "Epoch: 0001 Batch: 11750 /68439 loss = 0.157355 ppl = 1.17041\n",
      "Epoch: 0001 Batch: 11800 /68439 loss = 0.176843 ppl = 1.19344\n",
      "Epoch: 0001 Batch: 11850 /68439 loss = 0.155880 ppl = 1.16869\n",
      "Epoch: 0001 Batch: 11900 /68439 loss = 0.168760 ppl = 1.18384\n",
      "Epoch: 0001 Batch: 11950 /68439 loss = 0.193322 ppl = 1.21327\n",
      "Epoch: 0001 Batch: 12000 /68439 loss = 0.211507 ppl = 1.23554\n",
      "Epoch: 0001 Batch: 12050 /68439 loss = 0.183311 ppl = 1.20119\n",
      "Epoch: 0001 Batch: 12100 /68439 loss = 0.190856 ppl = 1.21028\n",
      "Epoch: 0001 Batch: 12150 /68439 loss = 0.149016 ppl = 1.16069\n",
      "Epoch: 0001 Batch: 12200 /68439 loss = 0.200617 ppl = 1.22216\n",
      "Epoch: 0001 Batch: 12250 /68439 loss = 0.163608 ppl = 1.17775\n",
      "Epoch: 0001 Batch: 12300 /68439 loss = 0.197277 ppl = 1.21808\n",
      "Epoch: 0001 Batch: 12350 /68439 loss = 0.177740 ppl = 1.19451\n",
      "Epoch: 0001 Batch: 12400 /68439 loss = 0.190943 ppl = 1.21039\n",
      "Epoch: 0001 Batch: 12450 /68439 loss = 0.135371 ppl = 1.14496\n",
      "Epoch: 0001 Batch: 12500 /68439 loss = 0.141962 ppl = 1.15253\n",
      "Epoch: 0001 Batch: 12550 /68439 loss = 0.180907 ppl = 1.1983\n",
      "Epoch: 0001 Batch: 12600 /68439 loss = 0.166286 ppl = 1.18091\n",
      "Epoch: 0001 Batch: 12650 /68439 loss = 0.195032 ppl = 1.21535\n",
      "Epoch: 0001 Batch: 12700 /68439 loss = 0.166184 ppl = 1.18079\n",
      "Epoch: 0001 Batch: 12750 /68439 loss = 0.225888 ppl = 1.25344\n",
      "Epoch: 0001 Batch: 12800 /68439 loss = 0.114907 ppl = 1.12177\n",
      "Epoch: 0001 Batch: 12850 /68439 loss = 0.164255 ppl = 1.17852\n",
      "Epoch: 0001 Batch: 12900 /68439 loss = 0.232334 ppl = 1.26154\n",
      "Epoch: 0001 Batch: 12950 /68439 loss = 0.230619 ppl = 1.25938\n",
      "Epoch: 0001 Batch: 13000 /68439 loss = 0.194364 ppl = 1.21454\n",
      "Epoch: 0001 Batch: 13050 /68439 loss = 0.200976 ppl = 1.2226\n",
      "Epoch: 0001 Batch: 13100 /68439 loss = 0.181937 ppl = 1.19954\n",
      "Epoch: 0001 Batch: 13150 /68439 loss = 0.141173 ppl = 1.15162\n",
      "Epoch: 0001 Batch: 13200 /68439 loss = 0.226950 ppl = 1.25477\n",
      "Epoch: 0001 Batch: 13250 /68439 loss = 0.165066 ppl = 1.17947\n",
      "Epoch: 0001 Batch: 13300 /68439 loss = 0.168308 ppl = 1.1833\n",
      "Epoch: 0001 Batch: 13350 /68439 loss = 0.234469 ppl = 1.26424\n",
      "Epoch: 0001 Batch: 13400 /68439 loss = 0.159737 ppl = 1.1732\n",
      "Epoch: 0001 Batch: 13450 /68439 loss = 0.146156 ppl = 1.15738\n",
      "Epoch: 0001 Batch: 13500 /68439 loss = 0.190256 ppl = 1.20956\n",
      "Epoch: 0001 Batch: 13550 /68439 loss = 0.195332 ppl = 1.21571\n",
      "Epoch: 0001 Batch: 13600 /68439 loss = 0.118509 ppl = 1.12582\n",
      "Epoch: 0001 Batch: 13650 /68439 loss = 0.176214 ppl = 1.19269\n",
      "Epoch: 0001 Batch: 13700 /68439 loss = 0.183090 ppl = 1.20092\n",
      "Epoch: 0001 Batch: 13750 /68439 loss = 0.137060 ppl = 1.1469\n",
      "Epoch: 0001 Batch: 13800 /68439 loss = 0.155570 ppl = 1.16832\n",
      "Epoch: 0001 Batch: 13850 /68439 loss = 0.177245 ppl = 1.19392\n",
      "Epoch: 0001 Batch: 13900 /68439 loss = 0.175373 ppl = 1.19169\n",
      "Epoch: 0001 Batch: 13950 /68439 loss = 0.175416 ppl = 1.19174\n",
      "Epoch: 0001 Batch: 14000 /68439 loss = 0.192309 ppl = 1.21204\n",
      "Epoch: 0001 Batch: 14050 /68439 loss = 0.196306 ppl = 1.2169\n",
      "Epoch: 0001 Batch: 14100 /68439 loss = 0.186431 ppl = 1.20494\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m all_valid_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(all_valid_target)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain###############\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(\"\\nTest###############\")\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# all_test_batch, all_test_target = make_batch_valid_test(test_psth, word2number_dict, 128, n_step)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# all_test_batch = torch.LongTensor(all_test_batch)  # list to tensor\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# all_test_target = torch.LongTensor(all_test_target)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# select_model_path = \"./models/nnlm_model_epoch10.ckpt\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# test(select_model_path)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_batch)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# output: [batch_size, n_class]\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    batch_size = 512 #batch size\n",
    "    n_step = 5\n",
    "    train_path = './data/train.txt' # the path of train dataset\n",
    "    valid_path = './data/valid.txt'\n",
    "    test_psth = './data/test.txt'\n",
    "\n",
    "    word2number_dict, number2word_dict = make_dict(train_path) #use the make_dict function to make the dict\n",
    "    print(\"The size of the dictionary is:\", len(word2number_dict))\n",
    "\n",
    "    n_class = len(word2number_dict)  #n_class (= dict size)\n",
    "\n",
    "    # prepare training set\n",
    "    all_input_batch, all_target_batch = make_batch(train_path, word2number_dict, batch_size, n_step)  # make the batch\n",
    "    print(\"The number of the train batch is:\", len(all_input_batch))\n",
    " \n",
    "    all_input_batch = torch.LongTensor(all_input_batch).to(device)   #list to tensor\n",
    "    all_target_batch = torch.LongTensor(all_target_batch).to(device)\n",
    "\n",
    "    # prepare validation set\n",
    "    all_valid_batch, all_valid_target = make_batch_valid_test(valid_path, word2number_dict, 128, n_step)\n",
    "    all_valid_batch = torch.LongTensor(all_valid_batch).to(device)  # list to tensor\n",
    "    all_valid_target = torch.LongTensor(all_valid_target).to(device)\n",
    "\n",
    "    print(\"\\nTrain###############\")\n",
    "    train()\n",
    "\n",
    "    # print(\"\\nTest###############\")\n",
    "    # all_test_batch, all_test_target = make_batch_valid_test(test_psth, word2number_dict, 128, n_step)\n",
    "    # all_test_batch = torch.LongTensor(all_test_batch)  # list to tensor\n",
    "    # all_test_target = torch.LongTensor(all_test_target)\n",
    "    # select_model_path = \"./models/nnlm_model_epoch10.ckpt\"\n",
    "    # test(select_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
