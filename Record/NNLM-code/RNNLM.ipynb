{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dict(path):\n",
    "    with open(path, 'r', encoding='utf-8') as fr:\n",
    "        text = fr.readlines()\n",
    "    text = [n.strip() for n in text]\n",
    "    text = ' '.join(text).split(\" \")\n",
    "    text = list(set(text))\n",
    "    word2number_dict = {}\n",
    "    number2word_dict = {}\n",
    "    for i,word in enumerate(text,4):\n",
    "        word2number_dict[word] = i\n",
    "        number2word_dict[i] = word\n",
    "    word2number_dict[\"<pad>\"] = 0\n",
    "    number2word_dict[0] = \"<pad>\"\n",
    "    word2number_dict[\"<unk_word>\"] = 1\n",
    "    number2word_dict[1] = \"<unk_word>\"\n",
    "    word2number_dict[\"<sos>\"] = 2\n",
    "    number2word_dict[2] = \"<sos>\"\n",
    "    word2number_dict[\"<eos>\"] = 3\n",
    "    number2word_dict[3] = \"<eos>\"\n",
    "\n",
    "    return word2number_dict, number2word_dict\n",
    "\n",
    "def make_batch(path, word2number_dict, batch_size, n_step):\n",
    "    def word2number(n):\n",
    "        try:\n",
    "            return word2number_dict[n]\n",
    "        except:\n",
    "            return 1   #<unk_word>\n",
    "\n",
    "    all_input_batch = []\n",
    "    all_target_batch = []\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as fr:\n",
    "        text = fr.readlines()\n",
    "    text = [line.strip() for line in text]\n",
    " \n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in text:\n",
    "     \n",
    "        wordlist = sen.split()\n",
    "        wordlist.insert(0, \"<sos>\")\n",
    "        wordlist.append(\"<eos>\")\n",
    "  \n",
    "        if(len(wordlist)<n_step):continue\n",
    "  \n",
    "        for i in range(len(wordlist)):\n",
    "            if(i+n_step>=len(wordlist)):break\n",
    "            input= []\n",
    "            for j in range(n_step):\n",
    "                input.append(word2number(wordlist[i+j]))\n",
    "            target =word2number(wordlist[i+n_step])\n",
    "   \n",
    "            input_batch.append(input)\n",
    "            target_batch.append(target)\n",
    "            \n",
    "    for i in range(0,len(input_batch),batch_size):\n",
    "        if i+batch_size>len(input_batch):\n",
    "            break\n",
    "        all_input_batch.append(input_batch[i:i+batch_size])\n",
    "        all_target_batch.append(target_batch[i:i+batch_size])\n",
    "  \n",
    "    return all_input_batch, all_target_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, X,hidden):\n",
    "        X = self.embed(X) # X: [batch_size, n_step , embed_dim]\n",
    "        X = X.transpose(0,1) # X: [n_step, batch_size, embed_dim]\n",
    "        output , hidden = self.rnn(X,hidden)\n",
    "        # output: [n_step, batch_size, hidden_dim]\n",
    "        # hidden: [1, batch_size, hidden_dim]\n",
    "        output = self.fc1(output[-1]) # 取最后一个时间步的输出 # output: [batch_size, hidden_dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dictionary is: 7615\n",
      "The number of the train batch is: 150\n",
      "\n",
      "Train###############\n",
      "RNNLM(\n",
      "  (embed): Embedding(7615, 16)\n",
      "  (rnn): RNN(16, 128)\n",
      "  (fc1): Linear(in_features=128, out_features=7615, bias=True)\n",
      ")\n",
      "Epoch: 0001 Batch: 50 /150 loss = 6.466872 ppl = 643.468\n",
      "Epoch: 0001 Batch: 100 /150 loss = 6.741197 ppl = 846.573\n",
      "Epoch: 0001 Batch: 150 /150 loss = 6.724743 ppl = 832.758\n",
      "Valid 5504 samples after epoch: 0001 loss = 6.497840 ppl = 663.706\n",
      "Epoch: 0002 Batch: 50 /150 loss = 6.217069 ppl = 501.232\n",
      "Epoch: 0002 Batch: 100 /150 loss = 6.531851 ppl = 686.668\n",
      "Epoch: 0002 Batch: 150 /150 loss = 6.523173 ppl = 680.735\n",
      "Valid 5504 samples after epoch: 0002 loss = 6.395941 ppl = 599.407\n",
      "Epoch: 0003 Batch: 50 /150 loss = 6.070329 ppl = 432.823\n",
      "Epoch: 0003 Batch: 100 /150 loss = 6.372880 ppl = 585.743\n",
      "Epoch: 0003 Batch: 150 /150 loss = 6.359722 ppl = 578.086\n",
      "Valid 5504 samples after epoch: 0003 loss = 6.301984 ppl = 545.654\n",
      "Epoch: 0004 Batch: 50 /150 loss = 5.920478 ppl = 372.59\n",
      "Epoch: 0004 Batch: 100 /150 loss = 6.214348 ppl = 499.87\n",
      "Epoch: 0004 Batch: 150 /150 loss = 6.208490 ppl = 496.95\n",
      "Valid 5504 samples after epoch: 0004 loss = 6.222361 ppl = 503.891\n",
      "Epoch: 0005 Batch: 50 /150 loss = 5.768497 ppl = 320.056\n",
      "Epoch: 0005 Batch: 100 /150 loss = 6.058050 ppl = 427.541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_input_batch, all_target_batch):\n\u001b[0;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 49\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# output: [batch_size, n_class]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target_batch)\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mRNNLM.forward\u001b[1;34m(self, X, hidden)\u001b[0m\n\u001b[0;32m     12\u001b[0m output , hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(X,hidden)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# output: [n_step, batch_size, hidden_dim]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# hidden: [1, batch_size, hidden_dim]\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 取最后一个时间步的输出 # output: [batch_size, hidden_dim]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chun\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_step = 5 # number of steps, n-1 in paper\n",
    "n_hidden = 128 # number of hidden size, h in paper\n",
    "m = 16 # embedding size, m in paper\n",
    "batch_size = 512 #batch size\n",
    "\n",
    "train_path = './data/train.txt' # the path of train dataset\n",
    "valid_path = './data/valid.txt'\n",
    "test_psth = './data/test.txt'\n",
    "\n",
    "word2number_dict, number2word_dict = make_dict(train_path) #use the make_dict function to make the dict\n",
    "print(\"The size of the dictionary is:\", len(word2number_dict))\n",
    "\n",
    "n_class = len(word2number_dict)  #n_class (= dict size)\n",
    "\n",
    "# prepare training set\n",
    "all_input_batch, all_target_batch = make_batch(train_path, word2number_dict, batch_size, n_step)  # make the batch\n",
    "print(\"The number of the train batch is:\", len(all_input_batch))\n",
    "\n",
    "all_input_batch = torch.LongTensor(all_input_batch).to(device)   #list to tensor\n",
    "all_target_batch = torch.LongTensor(all_target_batch).to(device)\n",
    "\n",
    "# prepare validation set\n",
    "all_valid_batch, all_valid_target = make_batch(valid_path, word2number_dict, 128, n_step)\n",
    "all_valid_batch = torch.LongTensor(all_valid_batch).to(device)  # list to tensor\n",
    "all_valid_target = torch.LongTensor(all_valid_target).to(device)\n",
    "\n",
    "print(\"\\nTrain###############\")\n",
    "learn_rate = 0.001\n",
    "all_epoch = 100 #the all epoch for training\n",
    "save_checkpoint_epoch = 10 # save a checkpoint per save_checkpoint_epoch epochs\n",
    "model = RNNLM(vocab_size=n_class, embed_dim=m, hidden_dim=n_hidden)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "# Training\n",
    "batch_number = len(all_input_batch)\n",
    "\n",
    "for epoch in range(all_epoch):\n",
    "    count_batch = 0\n",
    "    \n",
    "    \n",
    "    for input_batch, target_batch in zip(all_input_batch, all_target_batch):\n",
    "        \n",
    "        hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "        \n",
    "        output = model(input_batch , hidden)\n",
    "        # output: [batch_size, n_class]\n",
    "        loss = criterion(output, target_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ppl = math.exp(loss.item())\n",
    "        \n",
    "        if (count_batch + 1) % 50 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Batch:', '%02d' % (count_batch + 1), f'/{batch_number}',\n",
    "                    'loss =', '{:.6f}'.format(loss), 'ppl =', '{:.6}'.format(ppl))\n",
    "\n",
    "        count_batch += 1\n",
    "    \n",
    "    total_loss = 0\n",
    "    # valid after training one epoch      \n",
    "    total_valid = len(all_valid_target)*128\n",
    "    count_loss = 0\n",
    "    \n",
    "    hidden = torch.zeros(1, 128, n_hidden)\n",
    "    for input_batch, target_batch in zip(all_valid_batch, all_valid_target):\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(model(input_batch,hidden), target_batch)\n",
    "            total_loss += loss.item()\n",
    "            count_loss += 1\n",
    "        \n",
    "    print(f'Valid {total_valid} samples after epoch:', '%04d' % (epoch + 1), 'loss =',\n",
    "            '{:.6f}'.format(total_loss / count_loss),\n",
    "            'ppl =', '{:.6}'.format(math.exp(total_loss / count_loss)))\n",
    "\n",
    "    if (epoch+1) % save_checkpoint_epoch == 0:\n",
    "        torch.save(model, f'models/rnnlm_model_epoch{epoch+1}.ckpt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
