{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    "\n",
    "# pip install accelerate\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_file = \"./test.zh-en.news.json\"\n",
    "data = []\n",
    "with open(test_file,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "inputs = []\n",
    "for d in data:\n",
    "    inputs.append(d['translation']['zh'])\n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "for d in data:\n",
    "    outputs.append(d['translation']['en'])\n",
    "   \n",
    "with open(\"./official.txt\", \"w\") as f:\n",
    "    for item in outputs:\n",
    "        f.write(item.strip().replace(\"\\n\",'') + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm()\n",
      "        (post_attention_layernorm): Qwen2RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"./Qwen2-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make Batch: 100%|██████████| 1875/1875 [4:37:30<00:00,  8.88s/batch]     \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 64\n",
    "responses = []\n",
    "for i in tqdm(range(0,len(inputs),batch_size),desc=\"Make Batch\",unit=\"batch\"):\n",
    "    middle_inputs = []\n",
    "    for prompt in inputs[i:i+batch_size]:\n",
    "        four_shot = \"\"\"Zh:\"是否有途径处罚他\"\\nEn:\"Is there a way to punish him?\"\\n\\nZh:\"以免再次发生这样的事情\"\\nEn:\"So that such a thing won’t happen again.\"\\n\\nZh:\"用空气来洗手，哈口气判断疾病，精确搜寻雾霾来源 昨天晚上，在浙江大学2016年学术年会开幕式上，一系列脑洞大开的学术成果获得表彰。\"\\nEn:\"Washing hands with air, diagnosing disease through breath, and accurately seeking for source of smog; last night, a series of creative academic achievements were awarded at the opening ceremony of the 2016 Annual Academic Conference of Zhejiang University.\"\\n\\nZh:\"生物传感器：你的家庭体检医生\"\\nEn:\"Biosensor: your home physical examination doctor\"\\n\\n\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"you are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{four_shot}Zh:{prompt}\\En:\"},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        middle_inputs.append(text)\n",
    "\n",
    "    model_inputs = tokenizer(middle_inputs,return_tensors=\"pt\",padding = True)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "    batch_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    responses.extend(batch_responses)\n",
    "    \n",
    "with open(\"./responses.txt\", \"w\") as f:\n",
    "    for item in responses:\n",
    "        f.write(item.strip().replace(\"\\n\",'') + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batchsize have significant influence on the proformance of the model.  \n",
    "In the case of different batchsize, tokenizer need have different padding length.\n",
    "if the pad token is too long ,the orignal sentence will not get the well understand by model.\n",
    "***batch_size - 3***\n",
    "```output:batch_size = 3\n",
    "1:是否有途径处罚他\n",
    "1:system\n",
    "Translate the following content into English.Don't add any other information.\n",
    "user\n",
    "***是否有途径处罚他***\n",
    "assistant\n",
    "\"Does he have any other penalty?\"\n",
    "len of inputs_id:43\tlen of responses_id:56\n",
    "2:以免再次发生这样的事情\n",
    "2:system\n",
    "Translate the following content into English.Don't add any other information.\n",
    "user\n",
    "***以免再次发生这样的事情***\n",
    "assistant\n",
    "\"Please do not repeat anything else.\"\n",
    "len of inputs_id:43\tlen of responses_id:56\n",
    "3:又或者餐厅还要多久会准备好？可不可以帮我问下？\n",
    "3:system\n",
    "Translate the following content into English.Don't add any other information.\n",
    "user\n",
    "***又或者餐厅还要多久会准备好？可不可以帮我问下？***\n",
    "assistant\n",
    "Could you please tell me when the restaurant will be ready?\n",
    "len of inputs_id:43\tlen of responses_id:56\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name_or_path = \"../models/Qwen2-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,padding_side='left')\n",
    "\n",
    "def get_one_response(Input):\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"you are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{Input}\"},\n",
    "        ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    middle_inputs = []\n",
    "    middle_inputs.append(text)\n",
    "\n",
    "    model_inputs = tokenizer(middle_inputs,return_tensors=\"pt\",padding = True)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "    batch_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return batch_responses[0]\n",
    "\n",
    "\n",
    "def greet(Input):\n",
    "    return get_one_response(Input)\n",
    "\n",
    "demo = gr.Interface(fn=greet, \n",
    "                    inputs=[\"text\"], \n",
    "                    outputs=\"text\",\n",
    "                    title=\"ChatBot\",\n",
    "                    description=\"A ChatBot Based on Qwen2-0.5B-Instruction\",\n",
    "                    article=\"ShAn3003\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
