{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:23:37.643281Z",
     "iopub.status.busy": "2024-07-12T13:23:37.642791Z",
     "iopub.status.idle": "2024-07-12T13:23:37.648218Z",
     "shell.execute_reply": "2024-07-12T13:23:37.647157Z",
     "shell.execute_reply.started": "2024-07-12T13:23:37.643250Z"
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1720784760222,
     "user": {
      "displayName": "Xie Shan",
      "userId": "02149219329026083024"
     },
     "user_tz": -480
    },
    "id": "JAtvmdt3R-58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_jsonl(path:str):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f.readlines() if  line]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-07-12T13:23:40.795188Z",
     "iopub.status.busy": "2024-07-12T13:23:40.794682Z",
     "iopub.status.idle": "2024-07-12T13:23:40.798613Z",
     "shell.execute_reply": "2024-07-12T13:23:40.797772Z",
     "shell.execute_reply.started": "2024-07-12T13:23:40.795156Z"
    },
    "executionInfo": {
     "elapsed": 3459,
     "status": "ok",
     "timestamp": 1720784764364,
     "user": {
      "displayName": "Xie Shan",
      "userId": "02149219329026083024"
     },
     "user_tz": -480
    },
    "id": "0NE0_e8-SIaf",
    "outputId": "e6008302-1394-4f3b-bdf4-ea4497ec8d37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-12T13:23:41.396027Z",
     "iopub.status.busy": "2024-07-12T13:23:41.395526Z",
     "iopub.status.idle": "2024-07-12T13:23:41.615777Z",
     "shell.execute_reply": "2024-07-12T13:23:41.614918Z",
     "shell.execute_reply.started": "2024-07-12T13:23:41.395994Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1720784764364,
     "user": {
      "displayName": "Xie Shan",
      "userId": "02149219329026083024"
     },
     "user_tz": -480
    },
    "id": "VtPE9moNR-59",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(\"test.jsonl\")\n",
    "test_data = read_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T13:23:42.291825Z",
     "iopub.status.busy": "2024-07-12T13:23:42.291328Z",
     "iopub.status.idle": "2024-07-12T13:23:42.295785Z",
     "shell.execute_reply": "2024-07-12T13:23:42.294823Z",
     "shell.execute_reply.started": "2024-07-12T13:23:42.291791Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1720784766566,
     "user": {
      "displayName": "Xie Shan",
      "userId": "02149219329026083024"
     },
     "user_tz": -480
    },
    "id": "ZAshZ6jmR-5-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-07-12T13:23:43.122223Z",
     "iopub.status.busy": "2024-07-12T13:23:43.121698Z",
     "iopub.status.idle": "2024-07-12T13:23:44.682653Z",
     "shell.execute_reply": "2024-07-12T13:23:44.681888Z",
     "shell.execute_reply.started": "2024-07-12T13:23:43.122190Z"
    },
    "executionInfo": {
     "elapsed": 13464,
     "status": "ok",
     "timestamp": 1720784781786,
     "user": {
      "displayName": "Xie Shan",
      "userId": "02149219329026083024"
     },
     "user_tz": -480
    },
    "id": "b2txHdz0R-5-",
    "outputId": "d7e9e686-b391-44c4-c33d-c9e21719ea8a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"./Qwen2-0.5B\",padding_side = \"left\")\n",
    "model=AutoModelForCausalLM.from_pretrained(\"./Qwen2-0.5B\").to(device)\n",
    "\n",
    "# tokenizer=AutoTokenizer.from_pretrained(\"../Record/Qwen2-0.5B-Instruct\")\n",
    "# model=AutoModelForCausalLM.from_pretrained(\"../Record/Qwen2-0.5B-Instruct\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-12T13:25:53.231075Z",
     "iopub.status.busy": "2024-07-12T13:25:53.230478Z",
     "iopub.status.idle": "2024-07-12T13:33:04.128551Z",
     "shell.execute_reply": "2024-07-12T13:33:04.127568Z",
     "shell.execute_reply.started": "2024-07-12T13:25:53.231042Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [07:10<00:00, 10.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# 假设test_data是一个包含问题的列表\n",
    "# 假设tokenizer和model已经定义好并且在设备上\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# 这里设置batch_size\n",
    "batch_size = 32\n",
    "num_batches = (len(test_data) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    batch_data = test_data[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_msgs = []\n",
    "    for question in batch_data:\n",
    "        eight_shot = \"\"\"Question: Bert, Ernie, and Peggy collect stamps. Bert has four times as many stamps as Ernie, but Ernie has three times as many stamps as Peggy. If Peggy currently has 75 stamps in her collection, how many stamps does she need to add to her collection to have a collection as large as Bert's collection?\\nAnswer: Ernie has three times as many stamps as Peggy, or 3*75=<<3*75=225>>225 stamps.\\nBert has 4 times as many stamps as Ernie, or 4*225=<<4*225=900>>900 stamps.\\nThus, Peggy would need 900-75=<<900-75=825>>825 more stamps to have a collection as large as Bert's.\\n#### 825\\n\\n\"\"\"\n",
    "        msg = [\n",
    "            {\"role\": \"system\", \"content\": \"you are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{eight_shot}Question:{question['question']}\\n\\nAnswer:\"},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            msg,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        batch_msgs.append(text)\n",
    "    model_inputs = tokenizer(batch_msgs, padding=True, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    predictions.extend(responses)\n",
    "    del model_inputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-12T13:33:15.530909Z",
     "iopub.status.busy": "2024-07-12T13:33:15.530403Z",
     "iopub.status.idle": "2024-07-12T13:33:15.551915Z",
     "shell.execute_reply": "2024-07-12T13:33:15.551199Z",
     "shell.execute_reply.started": "2024-07-12T13:33:15.530878Z"
    },
    "id": "Icz5W8hBhjzm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "json_predictions = [{\"question\":question['question'],\"answer\": prediction} for prediction,question in zip(predictions,test_data)]\n",
    "\n",
    "with open('Base_without_CoT.json', 'w') as f:\n",
    "    json.dump(json_predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5375707,
     "sourceId": 8935372,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
